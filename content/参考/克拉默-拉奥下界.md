克拉默-拉奥下界（Cramér-Rao Lower Bound，简称CRLB）是统计学中的一个重要概念，用于描述估计量的最小方差。具体来说，对于一个无偏估计量，其方差不可能小于克拉默-拉奥下界。

给定一个随机样本 $X_1, X_2, \dots, X_n$，其联合概率密度函数为 $f(x_1, x_2, \dots, x_n; \theta)$，其中 $\theta$ 是一个未知参数。如果 $T$ 是 $\theta$ 的一个无偏估计量，那么 $T$ 的方差 $\text{Var}(T)$ 满足以下不等式：

$\text{Var}(T) \geq \frac{1}{I(\theta)}$

其中 $I(\theta)$ 是关于 $\theta$ 的费舍尔信息量（Fisher information），定义为：

$I(\theta) = E\left[ \left( \frac{\partial \log f(X; \theta)}{\partial \theta} \right)^2 \right]$

这里的期望值 $E[\cdot]$ 是对所有可能的 $X$ 取值的平均。

简而言之，克拉默-拉奥下界为我们提供了一个估计量方差的下限，这有助于我们评估不同估计量的效率。如果一个估计量的方差达到了这个下界，那么这个估计量被称为优效的。